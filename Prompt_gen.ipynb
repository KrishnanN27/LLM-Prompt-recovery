{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "875fb21e-dc64-48fa-912d-82b1b7bbd013",
   "metadata": {},
   "source": [
    "Installation and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1d10786-af9d-4e29-be9a-8e08bd843bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315e6ee5-041b-4e72-8836-6dce8d4d89a8",
   "metadata": {},
   "source": [
    "Loading the Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "76f059d1-6fc6-4f1b-bee6-3945f0164be3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(50258, 768)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define model name\n",
    "model_name = \"gpt2\"\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "if tokenizer.pad_token is None:  # Add [PAD] token if missing\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "model.resize_token_embeddings(len(tokenizer))  # Adjust for added tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693f9ae7-53a3-4458-91c2-19376292785a",
   "metadata": {},
   "source": [
    "Testing default model's Text Generation with Stopping Criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8f32cb6f-befa-4b35-93c9-94b883dfdcb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am from India, and I have never been to any country where you can't find a good beer.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, LogitsProcessorList, StoppingCriteriaList, StoppingCriteria\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "if tokenizer.pad_token is None:  # Ensure pad token exists\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Define input text\n",
    "input_text = \"I am from India,\"\n",
    "\n",
    "# Tokenize input text\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "# Define custom stopping criteria\n",
    "class EndTokenStoppingCriteria(StoppingCriteria):\n",
    "    \"\"\"Custom stopping criteria to stop generation after a sentence-ending token.\"\"\"\n",
    "    def __init__(self, tokenizer):\n",
    "        self.sentence_end_ids = tokenizer.convert_tokens_to_ids(['.', '!', '?'])\n",
    "\n",
    "    def __call__(self, input_ids, scores, **kwargs):\n",
    "        # Stop generation if the last token is a sentence-ending token\n",
    "        if input_ids[0, -1].item() in self.sentence_end_ids:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "# Define stopping criteria\n",
    "stopping_criteria = StoppingCriteriaList([\n",
    "    EndTokenStoppingCriteria(tokenizer)  # Stop at sentence-end tokens\n",
    "])\n",
    "\n",
    "# Generate output\n",
    "output = model.generate(\n",
    "    inputs.input_ids,\n",
    "    attention_mask=inputs.attention_mask,\n",
    "    max_new_tokens=50,  # Restrict new token generation\n",
    "    num_return_sequences=1,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    do_sample=True,\n",
    "    temperature=0.6,    # Lower randomness for focus\n",
    "    top_k=30,           # Reduce the pool of token choices\n",
    "    top_p=0.8,          # Increase nucleus sampling strictness\n",
    "    repetition_penalty=1.5,  # Strongly penalize repetition\n",
    "    stopping_criteria=stopping_criteria  # Stop after a complete sentence\n",
    ")\n",
    "\n",
    "# Decode and display the generated text\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bd89427a-38a2-47be-859f-c1bcbb5a80c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First few rows of the dataset:\n",
      "           id                                      original_text  \\\n",
      "0  lZGdiueMer  `` Well, there are healthier ways to tell me y...   \n",
      "1  DfTJVFKrUk  Rory ran his shaky fingers through his wife's ...   \n",
      "2  LmJvKranXK  As I made my way on foot across town to the Po...   \n",
      "3  PpnqXQAdGH  `` Hello. We come in peace.'' \\n \\n The first ...   \n",
      "4  qOeXTfqgAM  `` Karen, what the helllllll izzz...'' says my...   \n",
      "\n",
      "                                      rewrite_prompt  \\\n",
      "0  Rewrite the story where the writer asks the re...   \n",
      "1               Rewrite the essay as a dramatic play   \n",
      "2  Rewrite the story with all the themes and sett...   \n",
      "3  Rewrite the essay if the advanced aliens didn'...   \n",
      "4  Rewrite the story as a court room drama starri...   \n",
      "\n",
      "                                      rewritten_text  \n",
      "0  Well, there are healthier ways to tell me you ...  \n",
      "1  ## The Final Curtain\\n\\n[FADE IN]\\n\\n**Setting...  \n",
      "2  As I made my way through the Tatooine desert o...  \n",
      "3  `` Hello. We come in peace.''\\n\\nThe first enc...  \n",
      "4  The courtroom erupted in an uproar as District...  \n",
      "\n",
      "Columns in the dataset:\n",
      "Index(['id', 'original_text', 'rewrite_prompt', 'rewritten_text'], dtype='object')\n",
      "\n",
      "Dataset info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2400 entries, 0 to 2399\n",
      "Data columns (total 4 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   id              2400 non-null   object\n",
      " 1   original_text   2400 non-null   object\n",
      " 2   rewrite_prompt  2400 non-null   object\n",
      " 3   rewritten_text  2400 non-null   object\n",
      "dtypes: object(4)\n",
      "memory usage: 75.1+ KB\n",
      "None\n",
      "\n",
      "Basic statistics of numerical columns:\n",
      "                id                                      original_text  \\\n",
      "count         2400                                               2400   \n",
      "unique        2400                                               2400   \n",
      "top     UdnLmthfGs  The black bar on Marks forearm shrunk again. H...   \n",
      "freq             1                                                  1   \n",
      "\n",
      "                                           rewrite_prompt  \\\n",
      "count                                                2400   \n",
      "unique                                               2400   \n",
      "top     Rewrite the essay from a religious perspective...   \n",
      "freq                                                    1   \n",
      "\n",
      "                                           rewritten_text  \n",
      "count                                                2400  \n",
      "unique                                               2400  \n",
      "top     The black bar on Mark's forearm was a testamen...  \n",
      "freq                                                    1  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file\n",
    "file_path = \"train.csv\"  # Replace with the actual path to your file\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Display the first few rows to understand the content\n",
    "print(\"First few rows of the dataset:\")\n",
    "print(data.head())\n",
    "\n",
    "# Display the columns to see what features are available\n",
    "print(\"\\nColumns in the dataset:\")\n",
    "print(data.columns)\n",
    "\n",
    "# Get basic statistics and info about the dataset\n",
    "print(\"\\nDataset info:\")\n",
    "print(data.info())\n",
    "\n",
    "print(\"\\nBasic statistics of numerical columns:\")\n",
    "print(data.describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70450e6-7638-413e-b17c-296643d09569",
   "metadata": {},
   "source": [
    "Testing Default gpt2's ability to guess prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6c89f694-daef-4fe3-ad1e-229299de6947",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Prompt:\n",
      "Jimmy, I think you're getting the point. \n",
      "Jimmy: Â It was a little more difficult than I thought, and I've made a few mistakes, but I think I've changed a lot. \n",
      "Jimmy:\n",
      "\n",
      "Actual Prompt:\n",
      "Rewrite the story where the writer asks the reader to help with their essay and is instead surprised when the reader secretly has made dramatic improvements the essay without the original author knowing .\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "file_path = \"TRAIN.csv\" \n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Use the first row of the dataset\n",
    "original_text = data.loc[0, \"original_text\"]\n",
    "rewritten_text = data.loc[0, \"rewritten_text\"]\n",
    "actual_prompt = data.loc[0, \"rewrite_prompt\"]\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "if tokenizer.pad_token is None:  # Ensure pad token exists\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Prepare a refined input prompt\n",
    "input_text = (\n",
    "    f\"Task: Analyze the relationship between the original and rewritten text.\\n\"\n",
    "    f\"Original Text: {original_text}\\n\"\n",
    "    f\"Rewritten Text: {rewritten_text}\\n\"\n",
    "    f\"Question: What prompt might have caused this transformation?\\n\"\n",
    "    f\"Rewrite Prompt: \"\n",
    ")\n",
    "\n",
    "# Tokenize input\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "\n",
    "# Generate the output (the guessed prompt)\n",
    "output = model.generate(\n",
    "    inputs.input_ids,\n",
    "    attention_mask=inputs.attention_mask,\n",
    "    max_new_tokens=50,  # Limit length of generation\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_k=50,\n",
    "    top_p=0.9\n",
    ")\n",
    "\n",
    "# Decode the generated text\n",
    "generated_prompt = tokenizer.decode(output[0], skip_special_tokens=True).replace(input_text, \"\")\n",
    "print(\"Generated Prompt:\")\n",
    "print(generated_prompt.strip())\n",
    "\n",
    "# Print the actual prompt for comparison\n",
    "print(\"\\nActual Prompt:\")\n",
    "print(actual_prompt.strip())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d01298-0bb8-4560-a772-74981bc8386f",
   "metadata": {},
   "source": [
    "Data Preparation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "27cc65eb-834f-4d5d-8749-80538f2fed50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "# Load the dataset and sample 100 rows\n",
    "file_path = \"train.csv\"  \n",
    "data = pd.read_csv(file_path).head(2100)\n",
    "\n",
    "# Format the dataset for fine-tuning\n",
    "def format_data(row):\n",
    "    input_text = (\n",
    "        f\"Original Text: {row['original_text']}\\n\"\n",
    "        f\"Rewritten Text: {row['rewritten_text']}\\n\"\n",
    "        f\"Rewrite Prompt: \"\n",
    "    )\n",
    "    return {\"input_text\": input_text, \"output_text\": row[\"rewrite_prompt\"]}\n",
    "\n",
    "formatted_data = data.apply(format_data, axis=1).tolist()\n",
    "dataset = Dataset.from_list(formatted_data)\n",
    "\n",
    "# Split into training and validation sets\n",
    "train_test_split = dataset.train_test_split(test_size=0.2)\n",
    "train_dataset = train_test_split[\"train\"]\n",
    "eval_dataset = train_test_split[\"test\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8950de94-6de7-4cad-9c45-3c3dfe7bf9c4",
   "metadata": {},
   "source": [
    "Tokenization for Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "69486b39-d286-45c0-b490-252203d29418",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "788cc5c5a30a4704b33d1ca2718f6c45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1680 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95f175649a124c73b23e53c90e7874f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/420 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load tokenizer\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Set padding token\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    # Tokenize input text\n",
    "    model_inputs = tokenizer(\n",
    "        examples[\"input_text\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=512  # Ensure consistent max length\n",
    "    )\n",
    "    \n",
    "    # Tokenize target text (labels)\n",
    "    labels = tokenizer(\n",
    "        examples[\"output_text\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=512  # Ensure consistent max length\n",
    "    )[\"input_ids\"]\n",
    "    \n",
    "    # Replace padding tokens in labels with -100 (ignored by loss function)\n",
    "    labels = [\n",
    "        [(label if label != tokenizer.pad_token_id else -100) for label in label_list]\n",
    "        for label_list in labels\n",
    "    ]\n",
    "    \n",
    "    model_inputs[\"labels\"] = labels\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "tokenized_train = train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_eval = eval_dataset.map(tokenize_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31806464-3bde-4571-9256-9e67c7bcc1e3",
   "metadata": {},
   "source": [
    "Training Arguments Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1b834d7e-2a62-45ac-b290-0c681b43184d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt2-finetuned-rewrite\",\n",
    "    eval_strategy=\"no\",  \n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    save_steps=50,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    num_train_epochs=3,\n",
    "    save_total_limit=1,\n",
    "    load_best_model_at_end=False,  \n",
    "    fp16=True,  \n",
    "    report_to=\"none\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009a2570-1b79-4860-90f6-9a1ccac325ec",
   "metadata": {},
   "source": [
    "Data Collator for Sequence-to-Sequence Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9abb697a-636e-4b06-a556-bba81f30768b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "from transformers import Trainer\n",
    "\n",
    "# Define a data collator for sequence-to-sequence tasks\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    padding=\"longest\",\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_eval,\n",
    "    data_collator=data_collator  \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbbac39c-7408-4ed6-82c5-fb699b4c4ef4",
   "metadata": {},
   "source": [
    "Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7ecf1986-4867-4b03-b7ee-b4bd9b82a227",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2520' max='2520' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2520/2520 06:37, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>5.303600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>5.532900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>5.836600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>5.549400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>5.600300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>5.594200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>5.582100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>5.472500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>5.069100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>5.489500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>5.731100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>5.639300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>5.359300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>5.685200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>6.058900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>5.590900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>5.805800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>5.552600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>5.503600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>5.513900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>5.440800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>5.186300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>5.500300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>5.139100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>5.335500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>5.408400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>5.687300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>5.228200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>5.372600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>5.629300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>5.402500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>5.068100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>5.032200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>5.586600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>5.434300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>5.739600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>5.206100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>5.251600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>5.281000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>5.629800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>5.571100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>5.216000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>4.972600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>5.545700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>5.405900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>5.243000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>5.316000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>5.260600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>5.672600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>5.671500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>4.864100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>5.217100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>530</td>\n",
       "      <td>5.305900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>5.410800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>4.886100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>5.158600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>570</td>\n",
       "      <td>5.381500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>5.689900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>590</td>\n",
       "      <td>5.621000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>5.764000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>610</td>\n",
       "      <td>5.459200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>5.566300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>630</td>\n",
       "      <td>5.437700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>5.210600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>4.953700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>5.377700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>670</td>\n",
       "      <td>5.416900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>5.213800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>690</td>\n",
       "      <td>5.336700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>5.122000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>710</td>\n",
       "      <td>5.441000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>5.789700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>730</td>\n",
       "      <td>5.524400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>740</td>\n",
       "      <td>5.481400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>5.208200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>760</td>\n",
       "      <td>5.313000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>770</td>\n",
       "      <td>5.448900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>780</td>\n",
       "      <td>5.347000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>790</td>\n",
       "      <td>5.396300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>5.195400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>810</td>\n",
       "      <td>5.351800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>820</td>\n",
       "      <td>5.480400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>830</td>\n",
       "      <td>5.050100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>840</td>\n",
       "      <td>4.853000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>5.280600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>860</td>\n",
       "      <td>4.616000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>870</td>\n",
       "      <td>4.847100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>5.207900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>890</td>\n",
       "      <td>5.186600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>5.112600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>910</td>\n",
       "      <td>5.415000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>920</td>\n",
       "      <td>4.729600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>930</td>\n",
       "      <td>5.033100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>940</td>\n",
       "      <td>5.013600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>4.633600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>5.158500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>970</td>\n",
       "      <td>5.167400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>980</td>\n",
       "      <td>4.582200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>990</td>\n",
       "      <td>5.173500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>4.820800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1010</td>\n",
       "      <td>5.603300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1020</td>\n",
       "      <td>4.942400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1030</td>\n",
       "      <td>4.857900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1040</td>\n",
       "      <td>5.122300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>5.505900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1060</td>\n",
       "      <td>5.026100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1070</td>\n",
       "      <td>5.031300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1080</td>\n",
       "      <td>5.509400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1090</td>\n",
       "      <td>5.032300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>4.644100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1110</td>\n",
       "      <td>5.054200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1120</td>\n",
       "      <td>5.088300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1130</td>\n",
       "      <td>5.483000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1140</td>\n",
       "      <td>4.913200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>4.711700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1160</td>\n",
       "      <td>5.144800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1170</td>\n",
       "      <td>5.069600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1180</td>\n",
       "      <td>4.918900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1190</td>\n",
       "      <td>4.899100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>4.850300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1210</td>\n",
       "      <td>4.809400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1220</td>\n",
       "      <td>5.096400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1230</td>\n",
       "      <td>4.874300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1240</td>\n",
       "      <td>4.855800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>4.700400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1260</td>\n",
       "      <td>5.380800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1270</td>\n",
       "      <td>4.556800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1280</td>\n",
       "      <td>5.167300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1290</td>\n",
       "      <td>5.168900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>5.143200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1310</td>\n",
       "      <td>5.295900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1320</td>\n",
       "      <td>4.879400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1330</td>\n",
       "      <td>4.822000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1340</td>\n",
       "      <td>5.213100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>4.751500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1360</td>\n",
       "      <td>4.554600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1370</td>\n",
       "      <td>5.373200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1380</td>\n",
       "      <td>5.202400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1390</td>\n",
       "      <td>5.109000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>5.332600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1410</td>\n",
       "      <td>5.101100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1420</td>\n",
       "      <td>5.373500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1430</td>\n",
       "      <td>5.291600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1440</td>\n",
       "      <td>5.336200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>5.094000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1460</td>\n",
       "      <td>4.683500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1470</td>\n",
       "      <td>5.138000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1480</td>\n",
       "      <td>5.292800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1490</td>\n",
       "      <td>5.356000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>5.126700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1510</td>\n",
       "      <td>4.621600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1520</td>\n",
       "      <td>4.938100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1530</td>\n",
       "      <td>4.741200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1540</td>\n",
       "      <td>5.120000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>4.870200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1560</td>\n",
       "      <td>4.686000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1570</td>\n",
       "      <td>4.565300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1580</td>\n",
       "      <td>4.575200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1590</td>\n",
       "      <td>5.471100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>4.868300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1610</td>\n",
       "      <td>5.156100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1620</td>\n",
       "      <td>5.132000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1630</td>\n",
       "      <td>4.792800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1640</td>\n",
       "      <td>4.849900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>4.539600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1660</td>\n",
       "      <td>4.874700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1670</td>\n",
       "      <td>4.786800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1680</td>\n",
       "      <td>4.416700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1690</td>\n",
       "      <td>4.762700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>5.084200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1710</td>\n",
       "      <td>5.306900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1720</td>\n",
       "      <td>5.016500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1730</td>\n",
       "      <td>5.240800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1740</td>\n",
       "      <td>4.803400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>4.810100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1760</td>\n",
       "      <td>4.777500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1770</td>\n",
       "      <td>5.144100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1780</td>\n",
       "      <td>4.562900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1790</td>\n",
       "      <td>5.231000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>4.915400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1810</td>\n",
       "      <td>4.162500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1820</td>\n",
       "      <td>4.662200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1830</td>\n",
       "      <td>4.892200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1840</td>\n",
       "      <td>4.739100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>5.307000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1860</td>\n",
       "      <td>4.760600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1870</td>\n",
       "      <td>5.244800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1880</td>\n",
       "      <td>4.812600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1890</td>\n",
       "      <td>4.745900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>4.724400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1910</td>\n",
       "      <td>4.909600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1920</td>\n",
       "      <td>4.771700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1930</td>\n",
       "      <td>5.134400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1940</td>\n",
       "      <td>5.029200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>4.348500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1960</td>\n",
       "      <td>4.877100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1970</td>\n",
       "      <td>4.875400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1980</td>\n",
       "      <td>4.934500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1990</td>\n",
       "      <td>4.888500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>4.851200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2010</td>\n",
       "      <td>4.571800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020</td>\n",
       "      <td>4.743000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2030</td>\n",
       "      <td>4.686800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2040</td>\n",
       "      <td>5.073500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>4.619200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2060</td>\n",
       "      <td>4.525200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2070</td>\n",
       "      <td>4.780800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2080</td>\n",
       "      <td>4.926000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2090</td>\n",
       "      <td>4.681400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>4.825600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2110</td>\n",
       "      <td>5.161400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2120</td>\n",
       "      <td>4.438700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2130</td>\n",
       "      <td>4.974200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2140</td>\n",
       "      <td>4.862900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2150</td>\n",
       "      <td>4.946700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2160</td>\n",
       "      <td>4.613900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2170</td>\n",
       "      <td>4.794200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2180</td>\n",
       "      <td>4.841100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2190</td>\n",
       "      <td>5.266700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>4.794900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2210</td>\n",
       "      <td>4.449300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2220</td>\n",
       "      <td>5.160000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2230</td>\n",
       "      <td>4.175900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2240</td>\n",
       "      <td>4.883400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>4.805900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2260</td>\n",
       "      <td>4.359600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2270</td>\n",
       "      <td>4.752000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2280</td>\n",
       "      <td>5.116600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2290</td>\n",
       "      <td>5.253800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>5.114700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2310</td>\n",
       "      <td>4.579000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2320</td>\n",
       "      <td>4.448100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2330</td>\n",
       "      <td>5.005500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2340</td>\n",
       "      <td>4.853000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2350</td>\n",
       "      <td>5.196600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2360</td>\n",
       "      <td>4.761500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2370</td>\n",
       "      <td>4.925700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2380</td>\n",
       "      <td>4.877400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2390</td>\n",
       "      <td>4.766400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>5.256500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2410</td>\n",
       "      <td>4.778800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2420</td>\n",
       "      <td>4.620900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2430</td>\n",
       "      <td>5.177400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2440</td>\n",
       "      <td>4.629300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2450</td>\n",
       "      <td>4.713600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2460</td>\n",
       "      <td>4.799900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2470</td>\n",
       "      <td>4.888900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2480</td>\n",
       "      <td>5.209100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2490</td>\n",
       "      <td>4.761800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>4.920500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2510</td>\n",
       "      <td>5.142100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2520</td>\n",
       "      <td>5.072100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2520, training_loss=5.089531503404889, metrics={'train_runtime': 398.4998, 'train_samples_per_second': 12.647, 'train_steps_per_second': 6.324, 'total_flos': 1316911841280000.0, 'train_loss': 5.089531503404889, 'epoch': 3.0})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aecd4bc6-86fb-4dd0-afa9-6abe8970cb49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./2000-gpt2-finetuned-rewrite\\\\tokenizer_config.json',\n",
       " './2000-gpt2-finetuned-rewrite\\\\special_tokens_map.json',\n",
       " './2000-gpt2-finetuned-rewrite\\\\vocab.json',\n",
       " './2000-gpt2-finetuned-rewrite\\\\merges.txt',\n",
       " './2000-gpt2-finetuned-rewrite\\\\added_tokens.json',\n",
       " './2000-gpt2-finetuned-rewrite\\\\tokenizer.json')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"./2000-gpt2-finetuned-rewrite\")\n",
    "tokenizer.save_pretrained(\"./2000-gpt2-finetuned-rewrite\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f6081474-a427-4381-ab06-a3ea4fde5929",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, StoppingCriteria, StoppingCriteriaList\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class SentenceEndStoppingCriteria(StoppingCriteria):\n",
    "    \"\"\"Stops generation when a sentence-ending token is generated.\"\"\"\n",
    "    def __init__(self, tokenizer, sentence_end_ids):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.sentence_end_ids = sentence_end_ids\n",
    "\n",
    "    def __call__(self, input_ids, scores, **kwargs):\n",
    "        # Get the last token generated\n",
    "        last_token_id = input_ids[0, -1].item()\n",
    "        # If the last token is a sentence-ending token, stop generation\n",
    "        if last_token_id in self.sentence_end_ids:\n",
    "            return True\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "64f1a58e-3db9-43f7-a57a-636511d1d565",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the fine-tuned model and tokenizer\n",
    "model_path = \"./v2-gpt2-finetuned-rewrite\"  # Ensure this path is correct\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# Set device to CPU (since you're using CPU)\n",
    "device = torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Ensure pad token is set\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    model.resize_token_embeddings(len(tokenizer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "30ed4ec8-9264-43ce-9c59-26663c3a5b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_rewrite_prompt(original_text, rewritten_text, few_shot_examples=None):\n",
    "    \"\"\"\n",
    "    Generate a rewrite prompt based on the provided original and rewritten text.\n",
    "    \n",
    "    :param original_text: The original input text.\n",
    "    :param rewritten_text: The rewritten version of the text.\n",
    "    :param few_shot_examples: Optional list of dicts containing 'original', 'rewritten', and 'prompt'.\n",
    "    :return: Generated rewrite prompt.\n",
    "    \"\"\"\n",
    "    # Prepare few-shot examples if provided\n",
    "    few_shot_prompt = \"\"\n",
    "    if few_shot_examples:\n",
    "        for example in few_shot_examples:\n",
    "            few_shot_prompt += (\n",
    "                f\"Original Text: {example['original']}\\n\"\n",
    "                f\"Rewritten Text: {example['rewritten']}\\n\"\n",
    "                f\"Rewrite Prompt: {example['prompt']}\\n\\n\"\n",
    "            )\n",
    "\n",
    "    # Combine few-shot examples with the test case\n",
    "    test_input = (\n",
    "        few_shot_prompt +\n",
    "        f\"Original Text: {original_text}\\n\"\n",
    "        f\"Rewritten Text: {rewritten_text}\\n\"\n",
    "        f\"Rewrite Prompt: Rewrite\"\n",
    "    )\n",
    "\n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(test_input, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # Define sentence-ending token IDs\n",
    "    sentence_end_tokens = ['.', '!', '?']\n",
    "    sentence_end_ids = tokenizer.convert_tokens_to_ids(sentence_end_tokens)\n",
    "\n",
    "    # Initialize stopping criteria\n",
    "    stopping_criteria = StoppingCriteriaList([\n",
    "        SentenceEndStoppingCriteria(tokenizer, sentence_end_ids)\n",
    "    ])\n",
    "\n",
    "    # Generate output with stopping criteria\n",
    "    outputs = model.generate(\n",
    "        inputs.input_ids,\n",
    "        max_new_tokens=50,          # Generate up to 50 new tokens\n",
    "        num_return_sequences=1,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        do_sample=True,             # Enable sampling for diversity\n",
    "        temperature=0.7,            # Control randomness\n",
    "        top_k=30,                   # Limit the next token to the top 30\n",
    "        top_p=0.8,                  # Use nucleus sampling\n",
    "        repetition_penalty=1.5,     # Penalize repetition\n",
    "        stopping_criteria=stopping_criteria\n",
    "    )\n",
    "\n",
    "    # Decode the generated prompt\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    # Remove the input prompt from the output\n",
    "    generated_prompt = generated_text.replace(test_input, \"\").strip()\n",
    "    \n",
    "    # Ensure the prompt starts with \"Rewrite\"\n",
    "    if not generated_prompt.lower().startswith(\"rewrite\"):\n",
    "        generated_prompt = \"Rewrite \" + generated_prompt\n",
    "\n",
    "    # Ensure the prompt ends with a sentence-ending token\n",
    "    if not generated_prompt.endswith(('.', '!', '?')):\n",
    "        generated_prompt += \".\"\n",
    "\n",
    "    return generated_prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "44f51958-89c1-4b0c-874a-0c4a88ae9625",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Rewrite Prompt:\n",
      "Rewrite in of, story as essay and , is action novel or game you are that about setting your .\n",
      "Expected Prompt:\n",
      "Rewrite the sentence to make it more descriptive and lively.\n",
      "Match: â\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Define few-shot examples\n",
    "few_shot_examples = [\n",
    "    {\n",
    "        \"original\": \"The cat sat on the mat.\",\n",
    "        \"rewritten\": \"A lazy cat lounged on the soft mat.\",\n",
    "        \"prompt\": \"Rewrite the sentence with a more descriptive tone.\"\n",
    "    },\n",
    "    {\n",
    "        \"original\": \"The child ran to the park.\",\n",
    "        \"rewritten\": \"The excited child dashed toward the lively park.\",\n",
    "        \"prompt\": \"Rewrite the sentence to make it more vivid.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Define the test cases with expected prompts\n",
    "test_cases = [\n",
    "    {\n",
    "        \"original\": \"The dog chased the ball.\",\n",
    "        \"rewritten\": \"The playful dog ran after the ball with enthusiasm.\",\n",
    "        \"expected_prompt\": \"Rewrite the sentence to make it more descriptive and lively.\"\n",
    "    },\n",
    "    {\n",
    "        \"original\": \"The sun set over the mountains, painting the sky orange.\",\n",
    "        \"rewritten\": \"As the sun dipped behind the peaks, vibrant hues of orange and red lit up the sky.\",\n",
    "        \"expected_prompt\": \"Rewrite the sentence to emphasize the beauty of the sunset.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Define the test cases\n",
    "simple_test_case = {\n",
    "    \"original\": \"The dog chased the ball.\",\n",
    "    \"rewritten\": \"The playful dog ran after the ball with enthusiasm.\",\n",
    "    \"expected_prompt\": \"Rewrite the sentence to make it more descriptive and lively.\"\n",
    "}\n",
    "\n",
    "# Generate and print prompts for each test case\n",
    "for i, case in enumerate(test_cases, 1):\n",
    "    generated_prompt = generate_rewrite_prompt(\n",
    "    simple_test_case['original'],\n",
    "    simple_test_case['rewritten'],\n",
    "    few_shot_examples=few_shot_examples\n",
    ")\n",
    "print(\"Generated Rewrite Prompt:\")\n",
    "print(generated_prompt)\n",
    "print(\"Expected Prompt:\")\n",
    "print(simple_test_case['expected_prompt'])\n",
    "print(f\"Match: {'â' if generated_prompt.lower() == simple_test_case['expected_prompt'].lower() else 'â'}\")\n",
    "print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9db14201-6234-414f-aa7a-be7045db1997",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_rewrite_prompt_adjusted(original_text, rewritten_text, few_shot_examples=None):\n",
    "    \"\"\"\n",
    "    Generate a rewrite prompt with adjusted generation settings for better coherence.\n",
    "    \"\"\"\n",
    "    # Prepare few-shot examples if provided\n",
    "    few_shot_prompt = \"\"\n",
    "    if few_shot_examples:\n",
    "        for example in few_shot_examples:\n",
    "            few_shot_prompt += (\n",
    "                f\"Original Text: {example['original']}\\n\"\n",
    "                f\"Rewritten Text: {example['rewritten']}\\n\"\n",
    "                f\"Rewrite Prompt: {example['prompt']}\\n\\n\"\n",
    "            )\n",
    "\n",
    "    # Combine few-shot examples with the test case, ensuring it starts with \"Rewrite\"\n",
    "    test_input = (\n",
    "        few_shot_prompt +\n",
    "        f\"Original Text: {original_text}\\n\"\n",
    "        f\"Rewritten Text: {rewritten_text}\\n\"\n",
    "        f\"Rewrite Prompt: Rewrite\"\n",
    "    )\n",
    "\n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(test_input, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # Define sentence-ending token IDs\n",
    "    sentence_end_tokens = ['.', '!', '?']\n",
    "    sentence_end_ids = tokenizer.convert_tokens_to_ids(sentence_end_tokens)\n",
    "\n",
    "    # Initialize stopping criteria\n",
    "    stopping_criteria = StoppingCriteriaList([\n",
    "        SentenceEndStoppingCriteria(tokenizer, sentence_end_ids)\n",
    "    ])\n",
    "\n",
    "    # Generate output with adjusted settings\n",
    "    outputs = model.generate(\n",
    "        inputs.input_ids,\n",
    "        max_new_tokens=50,          # Generate up to 50 new tokens\n",
    "        num_return_sequences=1,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        do_sample=True,             # Enable sampling for diversity\n",
    "        temperature=0.5,            # Lower randomness for more focused output\n",
    "        top_k=30,                   # Restrict to top 30 tokens\n",
    "        top_p=0.85,                 # Use stricter nucleus sampling\n",
    "        repetition_penalty=2.0,     # Heavier penalty to prevent repetition\n",
    "        stopping_criteria=stopping_criteria\n",
    "    )\n",
    "\n",
    "    # Decode the generated prompt\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    # Remove the input prompt from the output\n",
    "    generated_prompt = generated_text.replace(test_input, \"\").strip()\n",
    "\n",
    "    # Ensure the prompt starts with \"Rewrite\"\n",
    "    if not generated_prompt.lower().startswith(\"rewrite\"):\n",
    "        generated_prompt = \"Rewrite \" + generated_prompt\n",
    "\n",
    "    # Ensure the prompt ends with a sentence-ending token\n",
    "    if not generated_prompt.endswith(('.', '!', '?')):\n",
    "        generated_prompt += \".\"\n",
    "\n",
    "    return generated_prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "be1902d1-0638-44b4-8976-509a5d220b52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Case 1 - Generated Prompt with Adjusted Settings:\n",
      "Rewrite and your characters is of in , that . are for story making, world as essay writing novel's yous humor about science character by but other time future action or setting where life at based an from instead be game adventure comedy writer can real events reality.\n",
      "Expected Prompt:\n",
      "Rewrite the sentence to make it more descriptive and lively.\n",
      "Match: â\n",
      "--------------------------------------------------\n",
      "Test Case 2 - Generated Prompt with Adjusted Settings:\n",
      "Rewrite , story novel that you are is for world- characters in life essay about adventure by game as all future reality's actions based from humor movie or where has not real but love if .\n",
      "Expected Prompt:\n",
      "Rewrite the sentence to emphasize the beauty of the sunset.\n",
      "Match: â\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Generate and print prompts for each test case with adjusted settings\n",
    "for i, case in enumerate(test_cases, 1):\n",
    "    generated_prompt = generate_rewrite_prompt_adjusted(\n",
    "        case['original'],\n",
    "        case['rewritten'],\n",
    "        few_shot_examples=few_shot_examples\n",
    "    )\n",
    "    print(f\"Test Case {i} - Generated Prompt with Adjusted Settings:\")\n",
    "    print(generated_prompt)\n",
    "    print(f\"Expected Prompt:\")\n",
    "    print(case['expected_prompt'])\n",
    "    print(f\"Match: {'â' if generated_prompt.lower() == case['expected_prompt'].lower() else 'â'}\")\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6aad3d-5881-4cbe-a95e-62e1e5e1e326",
   "metadata": {},
   "source": [
    "Conclusion:\n",
    "1. Insufficient Generalization\n",
    "The model has failed to generalize the task of inferring rewrite_prompt. \n",
    "\n",
    "Limited training data: With only 1000 examples, the model doesn't have enough variety to learn meaningful relationships.\n",
    "Complex task: Inferring a transformation prompt based on original_text and rewritten_text might require more nuanced understanding than what GPT-2 can achieve with minimal fine-tuning.\n",
    "\n",
    "2.Overfitting\n",
    "The model may have overfit to the small training dataset, memorizing patterns without understanding the task. Symptoms of overfitting include:\n",
    "\n",
    "Repeated or nonsensical outputs.\n",
    "Poor performance on unseen data or even on training examples.\n",
    "\n",
    "4. Model Limitations\n",
    "GPT-2 is a general-purpose model and It has fewer parameters (117M in the base model), limiting its ability to understand complex relationships or tasks.\n",
    "\n",
    "\n",
    "What to Do Next\n",
    "\n",
    "Alternatives to GPT-2\n",
    "a. Larger Transformer Models\n",
    "Metaâs LLaMA-2:\n",
    "\n",
    "Why: More parameters, better trained on diverse datasets.\n",
    "How to use: Open-source and fine-tunable, making it a strong candidate for your task.\n",
    "Example: Fine-tune LLaMA-2 using the Hugging Face transformers library.\n",
    "OpenAIâs GPT-3/4:\n",
    "\n",
    "Why: These are far more powerful and require no fine-tuning for most tasks.\n",
    "How to use: Prompt GPT-3 with your task in few-shot learning mode.\n",
    "Example: Pass a few examples of your dataset to GPT-3 and ask it to generate prompts.\n",
    "Googleâs T5 (Text-to-Text Transfer Transformer):\n",
    "\n",
    "Why: Specializes in text-to-text tasks like your rewrite prompt inference.\n",
    "\n",
    "\n",
    "             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396da737-4651-427b-ac6a-7a7ab300ac6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
